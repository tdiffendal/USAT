"risk_change_20_50",
"pct_risk_change_20_50",
"fs_fema_risk_deviation_2020",
"city_id",
"lat",
"long"))
# check the structure
str(flood.cities)
# Load census data for additional attributes
census.places <- read_csv("census/nhgis0070_csv/nhgis0070_ds239_20185_2018_place.csv")
# Some of the leading zeroes got trimmed from the
# city codes in the First Street city data
flood.cities %>%
mutate(length = nchar(city_id)) %>%
group_by(length) %>%
count()
# They should all be 7 characters in length
# create a new variable with the fixed city_id
flood.cities <- flood.cities %>%
mutate(city_id = case_when(nchar(city_id) == 6 ~ paste0("0", city_id),
nchar(city_id) == 7 ~ city_id))
census.places %>%
mutate(length = nchar(PLACEA)) %>%
group_by(length) %>%
count()
# All city IDs are 5 characters long, plus
# the state FIPS = 7 total characters
# create the city_id variable in the census data
census.places <- census.places %>%
mutate(city_id = paste0(STATEA, PLACEA))
# Join the census and flood data to get total populations
flood.cities <- flood.cities %>%
left_join(census.places, by = c("city_id" = "city_id")) %>%
select(city:state, city_id, total_population = AJWME001, total_properties:fs_fema_risk_deviation_2020)
# How many cities by state:
flood.cities %>%
group_by(state) %>%
count()
# 49 states, 28,160 cities (# rows)
# some general stats about the size of the cities by state
flood.cities %>%
group_by(state) %>%
summarise(avg_props = mean(total_properties),
median_props = median(total_properties),
min_props = min(total_properties),
max_props = max(total_properties))
#-----------------
flood.cities[is.na(flood.cities)] <- 0
#get number of states for naming
num_states = length(unique(flood.cities$state))
write_csv(flood.cities, paste0("cities/cities_", num_states, "_states_for_USAT_network_061220.csv"))
view(flood.cities$city)
print(flood.cities$city)
View(flood.cities)
chicago <- flood.cities %>%
filter(city == 'Chicago' )
la <- flood.cities %>%
filter(city == 'Los Angeles' )
ny <- flood.cities %>%
filter(city == 'New York' )
rand <- flood.cities %>%
filter(city == 'Rand' )
charles <- flood.cities %>%
filter(city == 'Charleston',
state == 'wv')
meyers <- flood.cities %>%
filter(city == 'Fort Myers' )
jack <- flood.cities %>%
filter(city == 'Jacksonville',
state == 'fl')
orleans <- flood.cities %>%
filter(city == 'New Orleans' )
View(la)
la <- flood.cities %>%
filter(city == 'Los Angeles',
state == 'ca')
View(flood.cities)
biggest <- flood.cities %>%
arrange(desc(total_population)) %>%
slice(1:10)
View(biggest)
View(flood.cities)
library(tidyverse)
library(ggalt)
library(sf)
library(viridis)
library(leaflet)
library(tigris)
library(janitor)
# Set the working directory
setwd("~/GitHub/USAT/first_street_flood_data")
#load data to be cleaned
flood.cities <- read_csv("cities/City_level_risk_FEMA_FSF_full.csv", col_types = "cciidididididicdd") %>%
`colnames<-` (c("city",
"state",
"total_properties",
"fema_risk",
"pct_fema_risk",
"risk_2020",
"pct_risk_2020",
"risk_2035",
"pct_risk_2035",
"risk_2050",
"pct_risk_2050",
"risk_change_20_50",
"pct_risk_change_20_50",
"fs_fema_risk_deviation_2020",
"city_id",
"lat",
"long")) %>%
mutate(fs_fema_risk_deviation_2020_pct = (fs_fema_risk_deviation_2020 * 100) / fema_risk)
str(flood.cities)
# Load census data for additional attributes
census.places <- read_csv("census/nhgis0070_csv/nhgis0070_ds239_20185_2018_place.csv")
# Some of the leading zeroes got trimmed from the
# city codes in the First Street city data
flood.cities %>%
mutate(length = nchar(city_id)) %>%
group_by(length) %>%
count()
# They should all be 7 characters in length
# create a new variable with the fixed city_id
flood.cities <- flood.cities %>%
mutate(city_id = case_when(nchar(city_id) == 6 ~ paste0("0", city_id),
nchar(city_id) == 7 ~ city_id))
census.places %>%
mutate(length = nchar(PLACEA)) %>%
group_by(length) %>%
count()
# All city IDs are 5 characters long, plus
# the state FIPS = 7 total characters
# create the city_id variable in the census data
census.places <- census.places %>%
mutate(city_id = paste0(STATEA, PLACEA))
# Join the census and flood data to get total populations
flood.cities <- flood.cities %>%
left_join(census.places, by = c("city_id" = "city_id")) %>%
select(city:state, city_id, total_population = AJWME001, total_properties:fs_fema_risk_deviation_2020_pct)
flood.cities %>%
group_by(state) %>%
count()
flood.cities %>%
group_by(state) %>%
summarise(avg_props = mean(total_properties),
median_props = median(total_properties),
min_props = min(total_properties),
max_props = max(total_properties))
#----------------------------
# Analysis of city data
#----------------------------
#turn nas to 0
flood.cities[is.na(flood.cities)] <- 0
#get number of states for naming
num_states = length(unique(flood.cities$state))
#write csv
write_csv(flood.cities, paste0("cities/cities_", num_states, "_states_for_USAT_network_061220.csv"))
flood.cities <- read.csv("cities/cities__states_for_USAT_network_061220.csv")
chicago <- flood.cities %>%
filter(city == 'Chicago' )
la <- flood.cities %>%
filter(city == 'Los Angeles',
state == 'ca')
ny <- flood.cities %>%
filter(city == 'New York' )
rand <- flood.cities %>%
filter(city == 'Rand' )
charles <- flood.cities %>%
filter(city == 'Charleston',
state == 'wv')
meyers <- flood.cities %>%
filter(city == 'Fort Myers' )
jack <- flood.cities %>%
filter(city == 'Jacksonville',
state == 'fl')
orleans <- flood.cities %>%
filter(city == 'New Orleans' )
biggest <- flood.cities %>%
arrange(desc(total_population)) %>%
slice(1:10)
rm(census.places)
View(biggest)
print(
(sum(biggest$fs_fema_risk_deviation_2020)
* 100)
/ sum(biggest$fema_risk)
)
flood.zips <- read.csv("ZIP/ZIPs_49_states_for_USAT_network_061220.csv")
View(flood.zips)
arizona <- flood.zips %>%
filter(state = 'az') %>%
arrange(fs_fema_difference_2020_pct)
arizona <- flood.zips %>%
filter(state == 'az') %>%
arrange(fs_fema_difference_2020_pct)
View(arizona)
arizona <- flood.zips %>%
filter(state == 'az',
fs_fema_difference_2020_pct < 0) %>%
count()
arizona <- flood.zips %>%
filter(state == 'az',
fs_fema_difference_2020_pct < 0) %>%
view(count())
arizona <- flood.zips %>%
filter(state == 'az',
fs_fema_difference_2020_pct < 0) %>%
view(count(fs_fema_difference_2020_pct))
arizona <- flood.zips %>%
filter(state == 'az',
fs_fema_difference_2020_pct < 0) %>%
count(fs_fema_difference_2020_pct)
arizona <- flood.zips %>%
filter(state == 'az',
fs_fema_difference_2020_pct < 0) %>%
print(count(fs_fema_difference_2020_pct))
arizona <- flood.zips %>%
filter(state == 'az',
fs_fema_difference_2020_pct < 0) %>%
count(fs_fema_difference_2020_pct)
View(arizona)
arizona <- flood.zips %>%
filter(state == 'az') %>%
count(fs_fema_difference_2020_pct < 0)
View(arizona)
arizona <- flood.zips %>%
filter(state == 'az') %>%
count(fs_fema_difference_2020_pct <= 0)
arizona <- flood.cities %>%
filter(state='az')
arizona <- flood.cities %>%
filter(state=='az')
View(arizona)
View(charles)
(sum(biggest$fs_fema_risk_deviation_2020)
print(
(sum(biggest$fs_fema_risk_deviation_2020)
* 100)
View(chicago)
View(meyers)
View(charles)
View(jack)
View(chicago)
View(charles)
View(la)
View(charles)
82738 - 80323
View(meyers)
View(charles)
View(orleans)
View(ny)
View(rand)
View(flood.cities)
femaest <- flood.cities %>%
arrange(desc(pct_fema_risk)) %>%
slice(1:10)
View(femaest)
femaest <- flood.cities %>%
filter(total_properties >= 1000)
arrange(desc(pct_fema_risk)) %>%
slice(1:10)
femaest <- flood.cities %>%
filter(total_properties >= 1000) %>%
arrange(desc(pct_fema_risk)) %>%
slice(1:10)
View(femaest)
femaest <- flood.cities %>%
filter(total_properties >= 10000) %>%
arrange(desc(pct_fema_risk)) %>%
slice(1:10)
View(femaest)
femaest <- flood.cities %>%
filter(total_properties >= 100000) %>%
arrange(desc(pct_fema_risk)) %>%
slice(1:10)
View(femaest)
femaest <- flood.cities %>%
filter(total_properties >= 100000) %>%
arrange(desc(pct_fema_risk)) %>%
count()
View(femaest)
femaest <- flood.cities %>%
filter(total_properties >= 100000)
View(femaest)
femaest <- flood.cities %>%
filter(total_properties >= 100000)
femaest <- flood.cities %>%
filter(total_properties >= 100000) %>%
arrange(desc(pct_fema_risk))
View(orleans)
View(chicago)
88338 - 858287
88338 - 80914
library(tidyverse)
library(ggalt)
library(sf)
library(viridis)
library(leaflet)
library(tigris)
library(janitor)
flood.cities <- read.csv("cities/cities_49_states_for_USAT_network_061220.csv")
interest <- flood.cities %>%
filter(city == 'Chicago' |
(city == 'Los Angeles' &
state == 'ca') |
city == 'New York' |
city == 'Rand' |
(city == 'Charleston' &
state == 'wv') |
city == 'Fort Myers' |
(city == 'Jacksonville' &
state == 'fl') |
city == 'New Orleans')
View(interest)
interest <- flood.cities %>%
filter(city == 'Chicago' |
(city == 'Los Angeles' &
state == 'ca') |
city == 'New York' |
city == 'Rand' |
(city == 'Charleston' &
state == 'wv') |
city == 'Fort Myers' |
(city == 'Jacksonville' &
state == 'fl') |
city == 'New Orleans') %>%
arrange()
View(interest)
interest <- flood.cities %>%
filter(city == 'Chicago' |
(city == 'Los Angeles' &
state == 'ca') |
city == 'New York' |
city == 'Rand' |
(city == 'Charleston' &
state == 'wv') |
city == 'Fort Myers' |
(city == 'Jacksonville' &
state == 'fl') |
city == 'New Orleans') %>%
arrange(city)
View(interest)
print(
(sum(biggest$fs_fema_risk_deviation_2020)
* 100)
/ sum(biggest$fema_risk)
)
biggest <- flood.cities %>%
arrange(desc(total_population)) %>%
slice(1:10)
## find by what percent the total # of houses at risk per fs is bigger/smaller than fema total#
print(
(sum(biggest$fs_fema_risk_deviation_2020)
* 100)
/ sum(biggest$fema_risk)
)
biggest <- flood.cities %>%
arrange(desc(total_properties)) %>%
slice(1:10)
View(biggest)
biggestPop <- flood.cities %>%
arrange(desc(total_population)) %>%
slice(1:10)
biggestProp <- flood.cities %>%
arrange(desc(total_properties)) %>%
slice(1:10)
rm(biggest)
View(biggestPop)
View(biggestProp)
library(tidyverse)
library(ggalt)
library(sf)
library(viridis)
library(leaflet)
library(tigris)
library(janitor)
# Set the working directory
setwd("~/GitHub/USAT/first_street_flood_data")
#load data to be cleaned
flood.cities <- read_csv("cities/City_level_risk_FEMA_FSF_full.csv", col_types = "cciidididididicdd") %>%
`colnames<-` (c("city",
"state",
"total_properties",
"fema_risk",
"pct_fema_risk",
"risk_2020",
"pct_risk_2020",
"risk_2035",
"pct_risk_2035",
"risk_2050",
"pct_risk_2050",
"risk_change_20_50",
"pct_risk_change_20_50",
"fs_fema_risk_deviation_2020",
"city_id",
"lat",
"long")) %>%
mutate(fs_fema_risk_deviation_2020_pct = (fs_fema_risk_deviation_2020 * 100) / fema_risk)
# check the structure
str(flood.cities)
# Load census data for additional attributes
census.places <- read_csv("census/nhgis0070_csv/nhgis0070_ds239_20185_2018_place.csv")
# Some of the leading zeroes got trimmed from the
# city codes in the First Street city data
flood.cities %>%
mutate(length = nchar(city_id)) %>%
group_by(length) %>%
count()
# They should all be 7 characters in length
# create a new variable with the fixed city_id
flood.cities <- flood.cities %>%
mutate(city_id = case_when(nchar(city_id) == 6 ~ paste0("0", city_id),
nchar(city_id) == 7 ~ city_id))
census.places %>%
mutate(length = nchar(PLACEA)) %>%
group_by(length) %>%
count()
# All city IDs are 5 characters long, plus
# the state FIPS = 7 total characters
# create the city_id variable in the census data
census.places <- census.places %>%
mutate(city_id = paste0(STATEA, PLACEA))
# Join the census and flood data to get total populations
flood.cities <- flood.cities %>%
left_join(census.places, by = c("city_id" = "city_id")) %>%
select(city:state, city_id, total_population = AJWME001, total_properties:fs_fema_risk_deviation_2020,fs_fema_risk_deviation_2020_pct, lat, long)
rm(census.places)
# How many cities by state:
flood.cities %>%
group_by(state) %>%
count()
# 49 states, 28,160 cities (# rows)
# some general stats about the size of the cities by state
flood.cities %>%
group_by(state) %>%
summarise(avg_props = mean(total_properties),
median_props = median(total_properties),
min_props = min(total_properties),
max_props = max(total_properties))
#----------------------------
# Analysis of city data
#----------------------------
#turn nas to 0
flood.cities[is.na(flood.cities)] <- 0
#get number of states for naming
num_states = length(unique(flood.cities$state))
#write csv
write_csv(flood.cities, paste0("cities/cities_", num_states, "_states_for_USAT_network_061220.csv"))
flood.cities <- read.csv("cities/cities_49_states_for_USAT_network_061220.csv")
#### pull out stats for cities we want to look at
interest <- flood.cities %>%
filter(city == 'Chicago' |
(city == 'Los Angeles' &
state == 'ca') |
city == 'New York' |
city == 'Rand' |
(city == 'Charleston' &
state == 'wv') |
city == 'Fort Myers' |
(city == 'Jacksonville' &
state == 'fl') |
city == 'New Orleans') %>%
arrange(city)
##### FS' arizona numbers low?
arizona <- flood.cities %>%
filter(state=='az')
#### biggest share according to Fema
femaest <- flood.cities %>%
filter(total_properties >= 100000) %>%
arrange(desc(pct_fema_risk)) %>%
count()
######################
## biggest states by pop
biggestPop <- flood.cities %>%
arrange(desc(total_population)) %>%
slice(1:10)
biggestProp <- flood.cities %>%
arrange(desc(total_properties)) %>%
slice(1:10)
View(interest)
biggestDif <- flood.cities %>%
mutate(pct_diff_2020 = risk_2020 - fema_risk ) %>%
arrange(pct_diff_2020) %>%
slice(1:10)
View(biggestDif)
biggestDif <- flood.cities %>%
mutate(pct_diff_2020 = pct_risk_2020 - pct_fema_risk ) %>%
arrange(desc(pct_diff_2020)) %>%
slice(1:10)
View(biggestDif)
biggestDif <- flood.cities %>%
mutate(pct_diff_2020 = pct_risk_2020 - pct_fema_risk ) %>%
filter(pct_diff_2020 < 100) %>%
arrange(desc(pct_diff_2020)) %>%
slice(1:10)
View(biggestDif)
biggestDif <- flood.cities %>%
mutate(pct_diff_2020 = pct_risk_2020 - pct_fema_risk ) %>%
filter(pct_diff_2020 < 100) %>%
arrange(desc(pct_diff_2020)) %>%
slice(1:20)
View(biggestDif)
biggestDif <- flood.cities %>%
mutate(pct_diff_2020 = pct_risk_2020 - pct_fema_risk ) %>%
filter(pct_diff_2020) %>%
arrange(desc(pct_diff_2020)) %>%
slice(1:50)
biggestDif <- flood.cities %>%
mutate(pct_diff_2020 = pct_risk_2020 - pct_fema_risk ) %>%
#filter(pct_diff_2020) %>%
arrange(desc(pct_diff_2020)) %>%
slice(1:50)
View(biggestDif)
